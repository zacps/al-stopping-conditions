{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from importlib import reload\n",
    "from functools import partial, lru_cache\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm, trange\n",
    "from joblib import delayed\n",
    "from modAL import batch\n",
    "from art.metrics import empirical_robustness\n",
    "from art.attacks.evasion import *\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from tabulate import tabulate\n",
    "from art.estimators.classification.scikitlearn import ScikitlearnSVC\n",
    "from sklearn.metrics.pairwise import paired_distances, euclidean_distances\n",
    "import scipy\n",
    "from tvregdiff.tvregdiff import TVRegDiff\n",
    "\n",
    "from ipynb.fs.defs import Bias\n",
    "from ipynb.fs.defs.Datasets import generateData_twoPills_2D, generateData_twoPills_noNoise_2D, plot_dataset_2D\n",
    "\n",
    "import libactive\n",
    "import libadversarial\n",
    "import libstop\n",
    "from libactive import MyActiveLearner, active_split\n",
    "from libadversarial import adversarial, uncertainty, random_batch, uncertainty_stop\n",
    "from libutil import ProgressParallel\n",
    "from libdatasets import *\n",
    "import librun\n",
    "from librun import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libdatasets; reload(libdatasets); from libdatasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap(func, *args, **kwargs):\n",
    "    wrapper = lambda: lru_cache()(func)(*args, **kwargs)\n",
    "    for attr in [attr for attr in dir(func) if not attr.startswith('__')]:\n",
    "        setattr(wrapper, attr, getattr(func, attr))\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = {\n",
    "    # Dataset fetchers should cache if possible\n",
    "    # Lambda wrapper required for function to be pickleable (sent to other threads via joblib)\n",
    "    \"datasets\": [\n",
    "        # Text classification\n",
    "        \n",
    "        # https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.6090&rep=rep1&type=pdf\n",
    "        (\"newsgroups_faith\", wrap(newsgroups, 1000, ('alt.atheism', 'soc.religion.christian'))),\n",
    "        (\"newsgroups_graphics\", wrap(newsgroups, 1000, ('comp.graphics', 'comp.windows.x'))),\n",
    "        (\"newsgroups_hardware\", wrap(newsgroups, 1000, ('comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware'))),\n",
    "        (\"newsgroups_sports_crypto\", wrap(newsgroups, 1000, ('rec.sport.baseball', 'sci.crypt'))),\n",
    "    \n",
    "        (\"rcv1\", wrap(rcv1, 1000)),\n",
    "        (\"webkb\", wrap(webkb, 1000)),\n",
    "        (\"spamassassin\", wrap(spamassassin, 1000)),\n",
    "        \n",
    "        # Image classification\n",
    "        (\"cifar10\", wrap(cifar10, 1000)),\n",
    "        (\"quickdraw\", wrap(quickdraw, 1000)),\n",
    "        \n",
    "        # General\n",
    "        (\"shuttle\", wrap(shuttle, 1000)),\n",
    "        (\"covertype\", wrap(covertype, 1000)),\n",
    "        (\"smartphone\", wrap(smartphone, 1000)),\n",
    "        (\"htru2\", wrap(htru2, 1000)),\n",
    "        #(\"malware\", wrap(malware, 1000)), # EXTREMELY SLOW FIT TIMES\n",
    "        (\"bidding\", wrap(bidding, 1000)),\n",
    "        (\"swarm\", wrap(swarm, 1000)),\n",
    "        (\"bank\", wrap(bank, 1000)),\n",
    "        \n",
    "        # Bio\n",
    "        (\"abalone\", wrap(abalone, 1000)),\n",
    "        (\"splice\", wrap(splice, 1000)),\n",
    "        \n",
    "        # Medical\n",
    "        (\"cardio\", wrap(cardio, 1000)),\n",
    "        (\"skin\", wrap(skin, 1000)),\n",
    "        \n",
    "    ],\n",
    "    \"dataset_mutators\": {\n",
    "        \"none\": (lambda *x, **kwargs: x),\n",
    "    },\n",
    "    \"methods\": [\n",
    "        (\"uncertainty\", partial(uncertainty_stop, n_instances=10)),\n",
    "    ],\n",
    "    \"models\": [\n",
    "        \"svm-linear\"\n",
    "    ],\n",
    "    \"meta\": {\n",
    "        \"dataset_size\": 1000,\n",
    "        \"labelled_size\": 10,\n",
    "        \"test_size\": {\n",
    "            \"newsgroups_faith\": 500,\n",
    "            \"newsgroups_graphics\": 500,\n",
    "            \"newsgroups_hardware\": 500,\n",
    "            \"newsgroups_sports_crypto\": 500,\n",
    "            \"*\": 0.5\n",
    "        },\n",
    "        \"n_runs\": 10,\n",
    "        \"ret_classifiers\": True,\n",
    "        \"ensure_y\": True,\n",
    "        \"stop_info\": True,\n",
    "        \"aggregate\": False,\n",
    "        \"stop_function\": (\"len1000\", lambda learner: learner.y_training.shape[0] >= 1000)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upper bound on total runtime:\n",
    "n_runs = 100\n",
    "n_datasets = 40\n",
    "n_query_methods = 2 # ?\n",
    "n_models = 1\n",
    "n_parameter_combinations = 1\n",
    "stop_size = 1000\n",
    "batch_size = 10\n",
    "\n",
    "total_runs = n_runs*n_datasets*n_query_methods*n_models*n_parameter_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'libactive' from 'C:\\\\Users\\\\Zac\\\\Programming\\\\python\\\\research\\\\libactive.py'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(libactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def pluralize(n, word):\n",
    "    if n == 1:\n",
    "        return '%d %s' % (n, word)\n",
    "        \n",
    "    return '%d %ss' % (n, word)\n",
    "        \n",
    "def format_duration(seconds):\n",
    "    if seconds == 0:\n",
    "        return \"now\"\n",
    "    \n",
    "    ONE_MINUTE = 60\n",
    "    ONE_HOUR = 60 * ONE_MINUTE\n",
    "    ONE_DAY = 24 * ONE_HOUR\n",
    "    ONE_YEAR = 365 * ONE_DAY\n",
    "    \n",
    "    units = (\n",
    "        (ONE_YEAR, 'year'),\n",
    "        (ONE_DAY, 'day'),\n",
    "        (ONE_HOUR, 'hour'),\n",
    "        (ONE_MINUTE, 'minute'),\n",
    "        (1, 'second'),\n",
    "    )\n",
    "        \n",
    "    r = []\n",
    "    for unit in units:\n",
    "        time_period, word = unit\n",
    "        if seconds >= time_period:\n",
    "            n = int(seconds / time_period)\n",
    "            r.append(pluralize(n, word))\n",
    "            seconds -= n * time_period\n",
    "    \n",
    "    return ' and'.join(', '.join(r).rsplit(',', 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import monotonic\n",
    "from sklearn.svm import SVC\n",
    "from libactive import expected_error\n",
    "from modAL.models import ActiveLearner\n",
    "from libadversarial import adversarial\n",
    "from art.attacks.evasion import DeepFool\n",
    "\n",
    "construct = []\n",
    "fit = []\n",
    "inference = []\n",
    "ee = []\n",
    "deepfool = []\n",
    "construct = [134.40000000009604, 146.79999999989377, 137.60000000002037, 131.40000000003056, 996.9999999999345, 6852.999999999884, 940.6000000000859, 1134.400000000096, 1222.0000000001164, 196.80000000007567, 1068.7999999998283]/total_runs*n_datasets\n",
    "fit = [80939.99999999141, 65000.0, 51239.99999999796, 45939.999999991414, 50920.00000000553, 53440.000000009604, 38760.00000000204, 410319.99999999243, 44059.999999990396, 8439.999999973224, 26802180.00000001]/total_runs*n_datasets\n",
    "# Assuming 1000 unlabelled instances are subsampled\n",
    "inference = [14376.525522657837, 12204.181258622519, 9850.36612543792, 7693.9403586968065, 8360.520826340678, 10416.765896641573, 6349.36374153124, 55546.66666666677, 7694.299167411302, 32.413793103152216]/total_runs*n_datasets\n",
    "deepfool = [176561999.99999988, 111873999.99999979, 108250000.0, 91280000.00000064, 100626000.00000021, 219467999.99999893, 77811999.9999999, 3773405999.999999, 156126000.00000024, 17155999.99999904]/total_runs*n_datasets\n",
    "ee = [25940000.000009604, 27200000.00001164, 18119999.999998983, 19380000.000001017, 35300000.00000655, 1293440000.0000098, 3439999.999991414, 313119999.999999, 39360000.00001513, 639999.9999848661]/total_runs*n_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cores = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 100 cores:\n",
      "  Estimated runtime for 100 runs on 40 datasets with 2 query methods: 591 years, 330 days, 13 hours, 56 minutes and 49 seconds\n",
      "  Without deepfool: 467 years, 343 days, 24 minutes and 46 secondsd\n",
      "  Without ee: 124 years, 5 days, 17 hours, 46 minutes and 56 secondsd\n",
      "  Without deepfool and ee: 18 days, 4 hours, 14 minutes and 53 secondsd\n"
     ]
    }
   ],
   "source": [
    "total_t_est = sum((sum(construct), sum(fit), sum(inference), sum(ee), sum(deepfool)))/len(construct)*n_datasets/n_cores\n",
    "t_est_no_ee = sum((sum(construct), sum(fit), sum(inference), sum(deepfool)))/len(construct)*n_datasets/n_cores\n",
    "t_est_no_deep = sum((sum(construct), sum(fit), sum(inference), sum(ee)))/len(construct)*n_datasets/n_cores\n",
    "t_est_no_deep_no_ee = sum((sum(construct), sum(fit), sum(inference)))/len(construct)*n_datasets/n_cores\n",
    "print(f\"Using {n_cores} cores:\")\n",
    "print(f\"  Estimated runtime for {n_runs} runs on {n_datasets} datasets with {n_query_methods} query methods: {format_duration(total_t_est)}\")\n",
    "print(f\"  Without deepfool: {format_duration(t_est_no_deep)}d\")\n",
    "print(f\"  Without ee: {format_duration(t_est_no_ee)}d\")\n",
    "print(f\"  Without deepfool and ee: {format_duration(t_est_no_deep_no_ee)}d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total fit time: 320 days, 53 minutes and 59 seconds\n",
      "Total inference time: 179 days, 16 hours, 19 minutes and 40 seconds\n",
      "Total ee time: 12866 years, 355 days, 14 hours, 31 minutes and 26 seconds\n",
      "Total deepfool time: 3409 years, 23 days, 11 minutes and 20 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total fit time: {format_duration(sum(fit))}\")\n",
    "print(f\"Total inference time: {format_duration(sum(inference))}\")\n",
    "print(f\"Total ee time: {format_duration(sum(ee))}\")\n",
    "print(f\"Total deepfool time: {format_duration(sum(deepfool))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options for running EE & deepfool:\n",
    "\n",
    "* Subsamble the unlabelled pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zac\\Programming\\python\\research\\libdatasets.py:197: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  dataset1 = pd.read_csv(\"Imitate/Datasets/shuttle.trn\", header=None, sep=\"\\s\")\n",
      "C:\\Users\\Zac\\Programming\\python\\research\\libdatasets.py:198: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  dataset2 = pd.read_csv(\"Imitate/Datasets/shuttle.tst\", header=None, sep=\"\\s\")\n"
     ]
    }
   ],
   "source": [
    "inference_sub = []\n",
    "ee_sub = []\n",
    "deepfool_sub = []\n",
    "for i, (name, dataset) in enumerate(matrix['datasets'][:10]):\n",
    "    x_all = getattr(libdatasets, name if not name.startswith(\"newsgroups\") else \"newsgroups\")(dataset_size=None)[0]\n",
    "    inference_sub.append(inference[i]/x_all.shape[0]*1000)\n",
    "    ee_sub.append(ee[i]/x_all.shape[0]*1000)\n",
    "    deepfool_sub.append(deepfool[i]/x_all.shape[0]*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14376.525522657837, 12204.181258622519, 9850.36612543792, 7693.9403586968065, 8360.520826340678, 10416.765896641573, 6349.36374153124, 55546.66666666677, 7694.299167411302, 32.413793103152216]\n",
      "[176561999.99999988, 111873999.99999979, 108250000.0, 91280000.00000064, 100626000.00000021, 219467999.99999893, 77811999.9999999, 3773405999.999999, 156126000.00000024, 17155999.99999904]\n",
      "[25940000.000009604, 27200000.00001164, 18119999.999998983, 19380000.000001017, 35300000.00000655, 1293440000.0000098, 3439999.999991414, 313119999.999999, 39360000.00001513, 639999.9999848661]\n"
     ]
    }
   ],
   "source": [
    "print(inference_sub)\n",
    "print(ee_sub)\n",
    "print(deepfool_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total fit time: 320 days, 53 minutes and 59 seconds\n",
      "Subsampled inference time: 1 day, 12 hours, 48 minutes and 45 seconds\n",
      "Subsampled ee time: 153 years, 87 days, 9 hours, 46 minutes and 39 seconds\n",
      "Subsampled deepfool time: 56 years, 114 days, 20 hours and 40 minutes\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total fit time: {format_duration(sum(fit))}\")\n",
    "print(f\"Subsampled inference time: {format_duration(sum(inference_sub))}\")\n",
    "print(f\"Subsampled ee time: {format_duration(sum(ee_sub))}\")\n",
    "print(f\"Subsampled deepfool time: {format_duration(sum(deepfool_sub))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cores=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2000 cores and subsampling:\n",
      "  Estimated runtime for 100 runs on 40 datasets with 2 query methods: 139 days, 15 hours, 39 minutes and 54 seconds\n",
      "  Without deepfool: 102 days, 6 hours, 43 minutes and 32 secondsd\n",
      "  Without ee: 37 days, 22 hours, 58 minutes and 41 secondsd\n",
      "  Without deepfool and ee: 14 hours, 2 minutes and 19 secondsd\n"
     ]
    }
   ],
   "source": [
    "total_t_est = sum((sum(construct), sum(fit), sum(inference_sub), sum(ee_sub), sum(deepfool_sub)))/len(construct)*n_datasets/n_cores\n",
    "t_est_no_ee = sum((sum(construct), sum(fit), sum(inference_sub), sum(deepfool_sub)))/len(construct)*n_datasets/n_cores\n",
    "t_est_no_deep = sum((sum(construct), sum(fit), sum(inference_sub), sum(ee_sub)))/len(construct)*n_datasets/n_cores\n",
    "t_est_no_deep_no_ee = sum((sum(construct), sum(fit), sum(inference_sub)))/len(construct)*n_datasets/n_cores\n",
    "print(f\"Using {n_cores} cores and subsampling:\")\n",
    "print(f\"  Estimated runtime for {n_runs} runs on {n_datasets} datasets with {n_query_methods} query methods: {format_duration(total_t_est)}\")\n",
    "print(f\"  Without deepfool: {format_duration(t_est_no_deep)}d\")\n",
    "print(f\"  Without ee: {format_duration(t_est_no_ee)}d\")\n",
    "print(f\"  Without deepfool and ee: {format_duration(t_est_no_deep_no_ee)}d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct=construct[:-1]\n",
    "#fit=fit[:-1]\n",
    "#inference=inference[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 10, 10, 10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(construct),len(fit),len(inference),len(ee),len(deepfool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[134.40000000009604, 146.79999999989377, 137.60000000002037, 131.40000000003056, 996.9999999999345, 6852.999999999884, 940.6000000000859, 1134.400000000096, 1222.0000000001164, 196.80000000007567, 1068.7999999998283]\n",
      "[80939.99999999141, 65000.0, 51239.99999999796, 45939.999999991414, 50920.00000000553, 53440.000000009604, 38760.00000000204, 410319.99999999243, 44059.999999990396, 8439.999999973224, 26802180.00000001]\n",
      "[270940.0000000096, 230000.0, 185640.00000000306, 145000.0, 6725320.00000001, 43739.99999999796, 38420.00000000553, 3332800.0000000065, 4279700.000000012, 1879.9999999828287, 270940.0000000096]\n",
      "[3327487451.9999976, 2108377403.9999962, 2040079500.0, 1720262880.0000122, 80944963164.00017, 921546131.9999955, 470840411.99999934, 226404359999.99994, 86839935342.00012, 995047999.9999443]\n",
      "[488865240.000181, 512611200.0002194, 341489519.9999808, 365235480.0000192, 28395814200.00527, 5431154560.000041, 20815439.999948047, 18787199999.99994, 21892701120.00842, 37119999.99912223, 31235205120.001183]\n"
     ]
    }
   ],
   "source": [
    "print(construct)\n",
    "print(fit)\n",
    "print(inference)\n",
    "print(ee)\n",
    "print(deepfool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-9893dfca847f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmonotonic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mexpected_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mee\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtotal_runs\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mn_datasets\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mstop_size\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Programming\\python\\research\\libactive.py\u001b[0m in \u001b[0;36mexpected_error\u001b[1;34m(learner, X, predict_proba, p_subsample)\u001b[0m\n\u001b[0;32m    654\u001b[0m                     \u001b[0mX_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_vstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m                 \u001b[0my_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_vstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 656\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    657\u001b[0m                 \u001b[0mcloned_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m                 \u001b[0mrefitted_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcloned_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_reduced\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zac\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zac\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport_vectors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_support\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual_coef_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_probA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m             self._probB, self.fit_status_ = libsvm.fit(\n\u001b[0m\u001b[0;32m    269\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m                 \u001b[0msvm_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for name, dataset in matrix['datasets'][10:]:\n",
    "    clf = SVC(kernel='linear', probability=True)\n",
    "    \n",
    "    start = monotonic()\n",
    "    X, y = dataset()\n",
    "    if X.dtype != np.float64:\n",
    "        X = X.astype(np.float64)\n",
    "    construct.append((monotonic()-start)*total_runs/n_datasets)\n",
    "    \n",
    "    start = monotonic()\n",
    "    clf.fit(X,y)\n",
    "    fit.append((monotonic()-start)*total_runs/n_datasets*stop_size/batch_size)\n",
    "    \n",
    "    x_all = getattr(libdatasets, name if not name.startswith(\"newsgroups\") else \"newsgroups\")(dataset_size=None)[0]\n",
    "    if x_all.dtype != np.float64:\n",
    "        x_all = x_all.astype(np.float64)\n",
    "    \n",
    "    start = monotonic()\n",
    "    clf.predict_proba(x_all)\n",
    "    inference.append((monotonic()-start)*total_runs/n_datasets*stop_size/batch_size)\n",
    "    \n",
    "    learner = ActiveLearner(estimator=clf, X_training=X, y_training=y)\n",
    "    \n",
    "    start = monotonic()\n",
    "    adversarial(learner, x_all[np.random.choice(x_all.shape[0], 1)], partial(DeepFool, verbose=False), n_instances=10)\n",
    "    deepfool.append((monotonic()-start)*total_runs/n_datasets*stop_size/batch_size*x_all.shape[0])\n",
    "    \n",
    "    start = monotonic()\n",
    "    expected_error(learner, x_all[np.random.choice(x_all.shape[0], 10)])\n",
    "    ee.append((monotonic()-start)*total_runs/n_datasets*stop_size/batch_size*x_all.shape[0]/10)\n",
    "    \n",
    "    print(f\"Finished {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(libactive); from libactive import expected_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = matrix['datasets'][0][1]()\n",
    "clf = SVC(kernel='linear', probability=True)\n",
    "clf.fit(X,y)\n",
    "x_all = newsgroups(None)[0]\n",
    "print(x_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_error(ActiveLearner(clf, X_training=X, y_training=y), x_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2, 3],\n",
       "        [4, 5, 6]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims([1,2,3], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MALWARE FIT DID NOT FINISH (07:30:30.xxx CPU time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset selection criteria\n",
    "\n",
    "* More than 3000 instances\n",
    "* More than 2 features\n",
    "* No missing values\n",
    "* Ideally easy to extract & preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = {\n",
    "    # Dataset fetchers should cache if possible\n",
    "    # Lambda wrapper required for function to be pickleable (sent to other threads via joblib)\n",
    "    \"datasets\": [\n",
    "        # Text classification\n",
    "        \n",
    "        # https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.6090&rep=rep1&type=pdf\n",
    "        (\"newsgroups_faith\", wrap(newsgroups, None, ('alt.atheism', 'soc.religion.christian'))),\n",
    "        (\"newsgroups_graphics\", wrap(newsgroups, None, ('comp.graphics', 'comp.windows.x'))),\n",
    "        (\"newsgroups_hardware\", wrap(newsgroups, None, ('comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware'))),\n",
    "        (\"newsgroups_sports_crypto\", wrap(newsgroups, None, ('rec.sport.baseball', 'sci.crypt'))),\n",
    "    \n",
    "        (\"rcv1\", wrap(rcv1, None)),\n",
    "        (\"webkb\", wrap(webkb, None)),\n",
    "        (\"spamassassin\", wrap(spamassassin, None)),\n",
    "        \n",
    "        # Image classification\n",
    "        (\"cifar10\", wrap(cifar10, None)),\n",
    "        (\"quickdraw\", wrap(quickdraw, None)),\n",
    "        \n",
    "        # General\n",
    "        (\"shuttle\", wrap(shuttle, None)),\n",
    "        (\"covertype\", wrap(covertype, None)), # fit takes a million years (1233s for 1000 instances)\n",
    "        (\"smartphone\", wrap(smartphone, None)),\n",
    "        (\"ida2016\", wrap(ida2016, None)), # HAS MISSING VALUES\n",
    "        (\"htru2\", wrap(htru2, None)),\n",
    "        (\"malware\", wrap(malware, None)),\n",
    "        (\"bidding\", wrap(bidding, None)),\n",
    "        (\"swarm\", wrap(swarm, None)),\n",
    "        \n",
    "        # Bio\n",
    "        (\"abalone\", wrap(abalone, )),\n",
    "        (\"splice\", wrap(splice, None)),\n",
    "        \n",
    "        # Medical\n",
    "        (\"cardio\", wrap(cardio, None)),\n",
    "        (\"skin\", wrap(skin, None)),\n",
    "        \n",
    "    ],\n",
    "    \"dataset_mutators\": {\n",
    "        \"none\": (lambda *x, **kwargs: x),\n",
    "    },\n",
    "    \"methods\": [\n",
    "        (\"uncertainty\", partial(uncertainty_stop, n_instances=10)),\n",
    "    ],\n",
    "    \"models\": [\n",
    "        \"svm-linear\"\n",
    "    ],\n",
    "    \"meta\": {\n",
    "        \"dataset_size\": 1000,\n",
    "        \"labelled_size\": 10,\n",
    "        \"test_size\": {\n",
    "            \"newsgroups_faith\": 500,\n",
    "            \"newsgroups_graphics\": 500,\n",
    "            \"newsgroups_hardware\": 500,\n",
    "            \"newsgroups_sports_crypto\": 500,\n",
    "            \"*\": 0.5\n",
    "        },\n",
    "        \"n_runs\": 10,\n",
    "        \"ret_classifiers\": True,\n",
    "        \"ensure_y\": True,\n",
    "        \"stop_info\": True,\n",
    "        \"aggregate\": False,\n",
    "        \"stop_function\": (\"len1000\", lambda learner: learner.y_training.shape[0] >= 1000)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_metrics = [\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    \n",
    "    \"uncertainty_average\",\n",
    "    \"uncertainty_min\",\n",
    "    \"uncertainty_max\",\n",
    "    \"uncertainty_variance\",\n",
    "    \"uncertainty_average_selected\",\n",
    "    \"uncertainty_min_selected\",\n",
    "    \"uncertainty_max_selected\",\n",
    "    \"uncertainty_variance_selected\",\n",
    "    \"entropy_max\",\n",
    "    \"n_support\",\n",
    "    \"contradictory_information\",\n",
    "    # slow\n",
    "    #\"expected_error\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zac\\Programming\\python\\research\\libdatasets.py:186: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  dataset1 = pd.read_csv(\"Imitate/Datasets/shuttle.trn\", header=None, sep=\"\\s\")\n",
      "C:\\Users\\Zac\\Programming\\python\\research\\libdatasets.py:187: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  dataset2 = pd.read_csv(\"Imitate/Datasets/shuttle.tst\", header=None, sep=\"\\s\")\n",
      "C:\\Users\\Zac\\Programming\\python\\research\\libdatasets.py:30: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  data = [dataset() for dataset in datasets]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset                     Instances    Classes    Features  Most common class    Least common class    Domain\n",
      "------------------------  -----------  ---------  ----------  -------------------  --------------------  --------\n",
      "newsgroups_faith                 1796          2      125145  15 56%               0 44%                 nlp\n",
      "newsgroups_graphics              1961          2      125145  5 50%                1 50%                 nlp\n",
      "newsgroups_hardware              1967          2      125145  2 50%                3 50%                 nlp\n",
      "newsgroups_sports_crypto         1985          2      125145  9 50%                11 50%                nlp\n",
      "rcv1                           804414          2       47236  0 53%                1 47%                 nlp\n",
      "webkb                            4199          4       22981  student 39%          project 12%           nlp\n",
      "spamassassin                     6051          2       50196  ham 69%              spam 31%              nlp\n",
      "cifar10                         60000         10        3072  0 10%                0 10%                 image\n",
      "quickdraw                      556217          4         784  4 29%                2 22%                 image\n",
      "shuttle                         58000          2          10  True 79%             False 21%             general\n",
      "covertype                      581012          7          54  2 49%                4 0%                  general\n",
      "smartphone                      10927         12         561  5 18%                8 0%                  general\n",
      "ida2016                         76000          2         170  neg 98%              pos 2%                computer\n",
      "htru2                           17898          2           8  0 91%                1 9%                  physical\n",
      "malware                          2955          1        1087  1 100%               1 100%                computer\n",
      "bidding                          6321          2          12  0 89%                1 11%                 general\n",
      "swarm                           24016          2        2400  0 50%                0 50%                 general\n",
      "abalone                          4177          3           8  M 37%                F 31%                 general\n",
      "splice                           3190          3         287  N 52%                EI 24%                general\n",
      "cardio                          70000          2           3  0 50%                1 50%                 general\n",
      "skin                           245057          2           3  2 79%                1 21%                 general\n"
     ]
    }
   ],
   "source": [
    "libdatasets.dataset_summary([data[0] for data in matrix['datasets']], [data[1] for data in matrix['datasets']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'librun' from 'C:\\\\Users\\\\Zac\\\\Programming\\\\python\\\\research\\\\librun.py'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(librun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    .p-Widget.jp-OutputPrompt.jp-OutputArea-prompt:empty {\n",
       "      padding: 0;\n",
       "      border: 0;\n",
       "    }\n",
       "    .p-Widget.jp-RenderedText.jp-OutputArea-output pre:empty {\n",
       "      display: none;\n",
       "    }\n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e8599752af469d93dc9e93e549a4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Experiment'), FloatProgress(value=0.0, max=21.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Run'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cache/classifiers/newsgroups_faith__none__uncertainty__svm-linear__dataset_size=1000__labelled_size=10__test_size=500__n_runs=10__ret_classifiers=True__ensure_y=True__stop_info=True__aggregate=False__stop_function=len1000.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Programming\\python\\research\\librun.py\u001b[0m in \u001b[0;36m__run_inner\u001b[1;34m(config, force_cache, force_run, backend, abort, metrics_measures)\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[1;34m\"time\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m         ]\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Programming\\python\\research\\librun.py\u001b[0m in \u001b[0;36m__read_classifiers\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cache/classifiers/newsgroups_faith__none__uncertainty__svm-linear__dataset_size=1000__labelled_size=10__test_size=500__n_runs=10__ret_classifiers=True__ensure_y=True__stop_info=True__aggregate=False__stop_function=len1000.pickle'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Programming\\python\\research\\librun.py\u001b[0m in \u001b[0;36m__run_inner\u001b[1;34m(config, force_cache, force_run, backend, abort, metrics_measures)\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"*\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Programming\\python\\research\\librun.py\u001b[0m in \u001b[0;36m__read_classifiers\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cache/classifiers/newsgroups_faith__none__uncertainty__svm-linear__dataset_size=1000__labelled_size=10__test_size=500__n_runs=10__ret_classifiers=True__ensure_y=True__stop_info=True__aggregate=False__stop_function=len1000.pickle'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-48fd8f238883>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcapture_metrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_run\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Programming\\python\\research\\librun.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(matrix, force_cache, force_run, backend, abort, workers, metrics)\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mworkers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msched_getaffinity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         results = ProgressParallel(\n\u001b[0;32m    112\u001b[0m             \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworkers\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mconfigurations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"n_runs\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Programming\\python\\research\\libutil.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         ) as self._pbar:\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mParallel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprint_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zac\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1046\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1048\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zac\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    864\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zac\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    782\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 784\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    785\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zac\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zac\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zac\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zac\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Programming\\python\\research\\librun.py\u001b[0m in \u001b[0;36m__run_inner\u001b[1;34m(config, force_cache, force_run, backend, abort, metrics_measures)\u001b[0m\n\u001b[0;32m    346\u001b[0m             )\n\u001b[0;32m    347\u001b[0m             \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmetrics_classifiers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m             \u001b[0mclassifiers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmetrics_classifiers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Programming\\python\\research\\librun.py\u001b[0m in \u001b[0;36m__write_classifiers\u001b[1;34m(config, classifiers)\u001b[0m\n\u001b[0;32m    372\u001b[0m                 \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m                 \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"cache/{config.serialize()}.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cache/classifiers/newsgroups_faith__none__uncertainty__svm-linear__dataset_size=1000__labelled_size=10__test_size=500__n_runs=10__ret_classifiers=True__ensure_y=True__stop_info=True__aggregate=False__stop_function=len1000.pickle'"
     ]
    }
   ],
   "source": [
    "results = librun.run(matrix, metrics=capture_metrics, force_run=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Learn to 1,000 instances. \n",
    "* Use a pool of as much data as possible for the dataset. \n",
    "* Start at 10+ensure_y instances\n",
    "* Use a validation set size of ???\n",
    "* Randomise the split each run, but use a seeded generator\n",
    "* Report results using autorank?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
