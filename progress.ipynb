{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tested a large number of adversarial methods http://130.216.217.104/notebooks/Zac%20testing/Adversarial%20comparison%20plots.ipynb\n",
    " \n",
    "  `DeepFool` & `HopSkipJump` perform well, followed by `Newtonfool` & `FGSM`, the rest mostly a mix except `Carlini & Wagner` (both variants) being a clear last\n",
    "\n",
    "* Tested a variety of datasets http://130.216.217.104/notebooks/Zac%20testing/real_datasets_plots.ipynb\n",
    "\n",
    "  Weird results on the skin dataset with deepfool\n",
    "\n",
    "* Tested the 'hardest' dataset from ALDataset http://130.216.217.104/notebooks/Zac%20testing/german.ipynb\n",
    "\n",
    "  Adversarial methods are about the same as uncertainty, random\n",
    "\n",
    "* Tested whether deepfool using logits (the 'right way) performed better http://130.216.217.104/notebooks/Zac%20testing/deepfool_logits.ipynb\n",
    "\n",
    "  Support for this [will be implemented in ART](https://github.com/Trusted-AI/adversarial-robustness-toolbox/pull/827)\n",
    "\n",
    "  Answer: Not really\n",
    "  \n",
    "* Get ThunderSVM building\n",
    "\n",
    "* Implement an uncertainty optimization query synthesis strategy\n",
    "\n",
    "* Port 'Efficient Active Learning of Halfspaces via Query Synthesis' to python (from matlab)\n",
    "\n",
    "  This algorithm is entirely dependant on the problem being realizable (a hypothesis with 100% accuracy exists) so isn't super interesting for my more real-world focused work.\n",
    "  \n",
    "* Test the uncertainty optimization strategy with different kernels\n",
    "\n",
    "  Linear & Polynomial work well. RBF seems to perform poorly, though not worse than random sampling which was also bad...\n",
    "  \n",
    "  The RBF kernel just overfit initially, then got forced to generalize. I'm not sure if there's a good way to prevent/detect overfitting?\n",
    "  \n",
    "  There probably is.\n",
    "  \n",
    "* Re-implement a poisoning query-synthesis strategy using the attack from SecML (previous attack was from ART)\n",
    "\n",
    "  Fixes the issues with the poisoning attack itself not doing what it is supposed to (I think) but still doesn't work well as a query strategy.\n",
    "  \n",
    "  I'm not sure why. My hypothesis was that it was only querying points of which the current classifier was certain, but I checked how often the attack's 'bad' label was the one returned from the oracle and it happened quite frequently (~50%).\n",
    "  \n",
    "* Came up with a few representativeness-only density measures (for pool-based query strategies)\n",
    "\n",
    "  Performed poorly.\n",
    "  \n",
    "* Started report writing (mostly an introduction)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 14/01 meeting\n",
    "\n",
    "* More runs on the logit testing for significance\n",
    "\n",
    "  Basically identical, existing results should be valid.\n",
    "  \n",
    "* Reading \"An overview and a benchmark of active learning for outlier detection with one-class classifiers\" for query strategy performance evaluation \n",
    "\n",
    "  Explored references, basically I'm not sure the continuous query-synthesis case really comes up. In most continuous datasets you're predicting outcomes where pool-based is the best model.\n",
    "  \n",
    "  Elsewhere in image & text classification it's too hard to label generated examples.\n",
    "  \n",
    "  Model stealing might be a use case... but idk.\n",
    "  \n",
    "* Tested more adversarial methods http://130.216.217.104/notebooks/Zac%20testing/adversarial_comparison.ipynb\n",
    "\n",
    "  FGM, DeepFool, HopSkipJump, Newtonfool, Boundary are the best methods.\n",
    "  \n",
    "* Testing a more randomised adversarial evasion query strategy\n",
    "\n",
    "  Done, not much of a difference on the few datasets I tried. I feel like this should be a good idea though...\n",
    "  \n",
    "* Read explainable active learning paper. Could be useful to analyze method performance if their techniques are general enough.\n",
    "\n",
    "  They were focused on human annotators, not explaining the AL algorithm itself.\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 20/01 meeting\n",
    "\n",
    "* Read papers on evaluating active learning methods\n",
    "\n",
    "  Lessons to learn here, I think mostly:\n",
    "  \n",
    "  * ~~Do X-fold validation~~\n",
    "  * ~~Test on multiple models~~\n",
    "  \n",
    "* Implement beam search\n",
    "\n",
    "  Done, run on car dataset\n",
    "  \n",
    "* Run beam search on german (on vm).\n",
    "\n",
    "  Doesn't perform that well surprisingly, might be better if optimized for ROC AUC?\n",
    "  \n",
    "  MUCH better, I'm curious if the ALDataset authors tried this? Might be worth mentioning.\n",
    "* Run more adversarial methods on the car dataset for a better comparison\n",
    "\n",
    "* Re-run skin with randomised splits\n",
    "\n",
    "  This does seem to be a legitimate case where deepfool performs better than uncertainty.\n",
    "  \n",
    "* Performance evaluation of in-progress active learning by evaluating the performance of past classifiers on the current labelled dataset\n",
    "  \n",
    "  Seems like it could work, more investigation needed.\n",
    "  \n",
    "  * More datasets\n",
    "  * Curve fitting for minimum detection?\n",
    "  * Which classifier to use? First? Average results?\n",
    "  * Does the initial classifier need to be trained on random data?\n",
    "      * How much data needs to be in the first set?\n",
    "  * It might be worthwhile to record the differences in the gradient as well\n",
    "    spikes caused by random non-informative samples tend to be quite sharp, \n",
    "    this might be a good was to discount them from being the true minimum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After ??\n",
    "\n",
    "* See if algorithm selects more of the less common biased bit\n",
    "\n",
    "  Couldn't get imitate's bias functions working straight away.\n",
    "\n",
    "* Noise: label flips\n",
    "\n",
    "  Seems to perform better, but not through selecting the noisy labels less frequently. I'm guessing it's just the 'natively better' performance\n",
    "  of the active learning methods making up for the loss of accuracy through noise?\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Implement other stopping criteria to compare\n",
    "\n",
    "  * Uncertainty\n",
    "    * Lowest confidence\n",
    "    * Average confidence\n",
    "    * ~~Confidence variance (selected points only https://www.aclweb.org/anthology/W10-0101.pdf)~~\n",
    "    \n",
    "  * Hyperplane similarity (KD)\n",
    "  \n",
    "    Possibly workable?\n",
    "\n",
    "  * Contradictory information\n",
    "  \n",
    "    https://www-sciencedirect-com.ezproxy.auckland.ac.nz/science/article/pii/S088523080700068X\n",
    "    \n",
    "    It's unclear if the contradictory information was actually used as a stopping criterion or if it was uncertainty, they don't really define it.\n",
    "    \n",
    "    Domain: NLP (NER, TC, shallow parsing)\n",
    "    \n",
    "    Models: SVM, max-entropy, bayesian logistic regression\n",
    "    \n",
    "    Test Datasets: [RCV1-v2](http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/lyrl2004_rcv1v2_README.htm), CoNLL 2003\n",
    "    \n",
    "  * ~~Number of new support vectors~~\n",
    "  \n",
    "    https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.6090&rep=rep1&type=pdf\n",
    "    \n",
    "    Domain: Document classification\n",
    "    \n",
    "    Models: SVM\n",
    "    \n",
    "    Test Datasets: 20newsgroups, Reuters\n",
    "    \n",
    "  * ~~Oracle accuracy~~\n",
    "  \n",
    "    https://www.aclweb.org/anthology/I08-1048.pdf\n",
    "    \n",
    "    Domain: NLP (NER, TC)\n",
    "    \n",
    "    Test Datasets: Ontonotes, WebKB\n",
    "  \n",
    "  * Expected error\n",
    "  \n",
    "    https://www.aclweb.org/anthology/I08-1048.pdf\n",
    "    \n",
    "    Domain: NLP (NER, TC)\n",
    "    \n",
    "    Test Datasets: Ontonotes, WebKB\n",
    "    \n",
    "  * Committe disagreement\n",
    "  \n",
    "    https://arxiv.org/pdf/2005.07402.pdf\n",
    "    \n",
    "    Domain: Generic\n",
    "    \n",
    "    Test Datasets: airfoil, power plant, protein, concrete, yacht\n",
    "    \n",
    "  * ~~Stop set~~\n",
    "  \n",
    "    (**PARITALLY** implemented)\n",
    "  \n",
    "    https://arxiv.org/pdf/1409.5165.pdf\n",
    "    \n",
    "    Domain: NLP (NER, TC)\n",
    "    \n",
    "    Models: SVM, max-entropy\n",
    "    \n",
    "    Test Datasets: spamassassin, trec05p1/ham25, Reuters-21578 Distribution 1.0 ModApte split, 20Newsgroups, WebKB, GENIA\n",
    "    \n",
    "  * Secondary regression model\n",
    "  \n",
    "    Domain: Drug targets\n",
    "    \n",
    "    Models: KBMF\n",
    "    \n",
    "    Test Datasets: KEGG BRITE, BRENDA, SuperTarget, DrugBank\n",
    "  \n",
    "    https://arxiv.org/pdf/1504.02406v1.pdf\n",
    "    \n",
    "  * Potential benefits of my method:\n",
    "    * No other set required (not even unlabelled)\n",
    "    * Low runtime cost\n",
    "    * Better active learning methods should directly improve the stopping condition (as picking less informative instances is less likely)\n",
    "    * Does not use confidence values, which may not work well with other query strategies? TEST THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Does it still work with a large unlabelled pool and not running until convergence? Take the full initial pool and only run until 500-1000 instances are labelled.\n",
    "\n",
    "  Yes, but not quite as well.\n",
    "  \n",
    "* Run stop results on other models (done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run noise on more datasets\n",
    "\n",
    "* Run evasion on biased datasets\n",
    "\n",
    "* Try unbalanced on more datasets (mnist done)\n",
    "\n",
    "* Collect more datasets for final stopping criteria evaluation\n",
    "\n",
    "* Ballpark CPU hours estimation\n",
    "\n",
    "* Apply for cluster access\n",
    "\n",
    "* Report writting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to do before cluster runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Move stopping condition evaluation into `active_learn2`. This stopps us from having to store all previous classifiers (memory intensive)\n",
    "* Add checkpointing so that an individual active learning run can be interrupted and resumed\n",
    "* Add checkpointing so that (dis) aggregated runs can be resumed part way through\n",
    "* Add a mechanism to split the configuration list into chunks for multiple jobs\n",
    "\n",
    "--\n",
    "\n",
    "* More datasets\n",
    "\n",
    "--\n",
    "\n",
    "* Runtime estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library roadmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base features (acceptance criteria):\n",
    "\n",
    "* Can accept a job matrix and output a list of configurations\n",
    "  * Settings can be applied to specific configurations or groups of configurations\n",
    "  * Specific combinations can be excluded\n",
    "* Can cache jobs on disk and retrieve their results when the same configuration is requested\n",
    "* Can parallelize experiments and/or repeated runs across CPU cores\n",
    "\n",
    "Extensions (if time allows):\n",
    "\n",
    "* Progress reporting\n",
    "* Completion/failure notifications\n",
    "* Disparate job queueing (i.e. separate invocations should check if compute resources are available and wait for existing jobs to complete if they are not)\n",
    "* Resource usage reporting (wall time, max resident set, average resident set, GPU usage)\n",
    "\n",
    "Hard extensions:\n",
    "\n",
    "* Interrupts/resumes on individual workers or on the whole job\n",
    "* Parallelization across multiple machines (SSH?)\n",
    "* HPC (Nectar/NeSI) support\n",
    "* Cache invalidation on source code change (including deps)\n",
    "\n",
    "\n",
    "* Stage (sequential)\n",
    "  * Experiment (parallel)\n",
    "    * Run (parallel)\n",
    "      * Operation (sequential)\n",
    "\n",
    "After each $thing is a suspension point "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Licensing for code I've written. Do I own it? Can I publish it to Github? After the review period (double blind). Under what license?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "* Are the adversarial methods robust to batch-mode operation even without specialisation? How does the non-batch mode uncertainty strategy compare? (Try with large ~50 batch sizes)\n",
    "\n",
    "  Can it perform better with the batch-mode adjustment? Seems like a no.\n",
    "* Analyze deepfool results to see if it generates reasonable examples\n",
    "* Add support for ThunderSVM into adversarial-robustness-toolbox\n",
    "* Figure out why deepfool did strangely well on the skin dataset \n",
    "* Come up with more query strategies? Synthesis or pool-based?\n",
    "* Try to find practical general query-synthesis strategies to compare my uncertainty based approach to\n",
    "\n",
    "  The main example of query synthesis in the wild is the robot-scientist, but it didn't use active learning in any of the traditional senses.\n",
    "  \n",
    "* Experiment with more (synthesis) algorithms in [libact](https://github.com/ntucllab/libact)?\n",
    "* Applications (datasets) for query synthesis  \n",
    "* Histogram generation per-feature, differentiating between AL after 100 instances-ish and the full distribution.\n",
    "\n",
    "  Would be interesting to see if some features become particularlly biased.\n",
    "  \n",
    "* Look at stopping criteria in more detail, might be something I could contribute to\n",
    "\n",
    "  See [14, 34, 45] of http://ceur-ws.org/Vol-1924/ialatecml_paper0.pdf\n",
    "  \n",
    "* Try to replicate results that the number of initialled labelled points doesn't really matter. Forget which paper this was from... one of the evaluation ones.\n",
    "\n",
    "* Try on (entirely, sans validation set) biased data or imbalanced (80/20 class distribution)\n",
    "\n",
    "  I don't think the adverdarial methods will do any better here but... whatever, we'll see.\n",
    "  \n",
    "* Query-synthesis active learning might be more viable with a 'null class'\n",
    "\n",
    "  Allow the oracle to classify images as neither of the 'of interest' classes might help with ambiguous samples? Perhaps the algorithm could be dissuaded \n",
    "  from querying points that are likely to be not of interest? Train a second classifier on those perhaps?\n",
    "  \n",
    "* TODO: Perform method comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
