{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tested a large number of adversarial methods http://130.216.217.104/notebooks/Zac%20testing/Adversarial%20comparison%20plots.ipynb\n",
    " \n",
    "  `DeepFool` & `HopSkipJump` perform well, followed by `Newtonfool` & `FGSM`, the rest mostly a mix except `Carlini & Wagner` (both variants) being a clear last\n",
    "\n",
    "* Tested a variety of datasets http://130.216.217.104/notebooks/Zac%20testing/real_datasets_plots.ipynb\n",
    "\n",
    "  Weird results on the skin dataset with deepfool\n",
    "\n",
    "* Tested the 'hardest' dataset from ALDataset http://130.216.217.104/notebooks/Zac%20testing/german.ipynb\n",
    "\n",
    "  Adversarial methods are about the same as uncertainty, random\n",
    "\n",
    "* Tested whether deepfool using logits (the 'right way) performed better http://130.216.217.104/notebooks/Zac%20testing/deepfool_logits.ipynb\n",
    "\n",
    "  Support for this [will be implemented in ART](https://github.com/Trusted-AI/adversarial-robustness-toolbox/pull/827)\n",
    "\n",
    "  Answer: Not really\n",
    "  \n",
    "* Get ThunderSVM building\n",
    "\n",
    "* Implement an uncertainty optimization query synthesis strategy\n",
    "\n",
    "* Port 'Efficient Active Learning of Halfspaces via Query Synthesis' to python (from matlab)\n",
    "\n",
    "  This algorithm is entirely dependant on the problem being realizable (a hypothesis with 100% accuracy exists) so isn't super interesting for my more real-world focused work.\n",
    "  \n",
    "* Test the uncertainty optimization strategy with different kernels\n",
    "\n",
    "  Linear & Polynomial work well. RBF seems to perform poorly, though not worse than random sampling which was also bad...\n",
    "  \n",
    "* Re-implement a poisoning query-synthesis strategy using the attack from SecML (previous attack was from ART)\n",
    "\n",
    "  Fixes the issues with the poisoning attack itself not doing what it is supposed to (I think) but still doesn't work well as a query strategy.\n",
    "  \n",
    "  I'm not sure why. My hypothesis was that it was only querying points of which the current classifier was certain, but I checked how often the attack's 'bad' label was the one returned from the oracle and it happened quite frequently (~50%).\n",
    "  \n",
    "* Came up with a few representativeness-only density measures (for pool-based query strategies)\n",
    "\n",
    "  Performed poorly.\n",
    "  \n",
    "* Started report writing (mostly an introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test more adversarial methods http://130.216.217.104/notebooks/Zac%20testing/adversarial_comparison.ipynb\n",
    "* More runs on the logit testing for significance\n",
    "* Add support for ThunderSVM into adversarial-robustness-toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Figure out why deepfool did strangely well on the skin dataset \n",
    "* Implement beam search?\n",
    "* Come up with more query strategies? Synthesis or pool-based?\n",
    "* Try to find practical general query-synthesis strategies to compare my uncertainty based approach to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
