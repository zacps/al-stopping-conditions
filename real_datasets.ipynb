{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning on Real Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from importlib import reload\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm, trange\n",
    "from joblib import delayed\n",
    "from modAL import batch\n",
    "from art.metrics import empirical_robustness\n",
    "\n",
    "from ipynb.fs.defs import Bias\n",
    "from ipynb.fs.defs.Datasets import generateData_twoPills_2D, generateData_twoPills_noNoise_2D, plot_dataset_2D\n",
    "\n",
    "import libactive\n",
    "import libadversarial\n",
    "from libactive import MyActiveLearner, active_split\n",
    "from libadversarial import fgm, deepfool, uncertainty_id, random_batch\n",
    "from libutil import ProgressParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def banknote():\n",
    "    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt'\n",
    "    dataset = pd.read_csv(url, header=None)\n",
    "    y = dataset[4].to_numpy()\n",
    "    isInB = np.array([dataset.to_numpy()[i,0]>0.32 for i in range(len(dataset))])\n",
    "    isInB = isInB.reshape(len(isInB), 1)\n",
    "    X = dataset.drop([4], axis=1).to_numpy()\n",
    "    #pca = PCA(n_components=21).fit(X)\n",
    "    #X = pca.transform(X)\n",
    "    X = np.append(X, isInB, axis=1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def bias_banknote(data, labels):\n",
    "    isInB = data[:,-1]\n",
    "    X = data[isInB==1]\n",
    "    X = X[:, 0:(len(data[0])-1)]\n",
    "    y = labels[isInB==1]\n",
    "    return X, y\n",
    "\n",
    "def haberman():\n",
    "    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt'\n",
    "    dataset = pd.read_csv(url, header=None)\n",
    "    \n",
    "    y = dataset[4].to_numpy()\n",
    "    X = dataset.drop([4], axis=1).to_numpy()\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def digits():\n",
    "    digits = datasets.load_digits()\n",
    "    n_samples = len(digits.images)\n",
    "    data = digits.images.reshape((n_samples, -1))\n",
    "    return data, digits.target\n",
    "\n",
    "def abalone():\n",
    "    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data'\n",
    "    dataset = pd.read_csv(url, header=None)\n",
    "    y = dataset[0].to_numpy()\n",
    "    isInB = np.array([dataset.to_numpy()[i,6]<0.144 for i in range(len(dataset))])\n",
    "    isInB = isInB.reshape(len(isInB), 1)\n",
    "    X = dataset.drop([0,6], axis=1).to_numpy()\n",
    "    X = np.append(X, isInB, axis=1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def bias_abalone(data, labels):\n",
    "    isInB = data[:,-1]\n",
    "    X = data[isInB==1]\n",
    "    X = X[:, 0:(len(data[0])-1)]\n",
    "    y = labels[isInB==1]\n",
    "    return X, y\n",
    "\n",
    "def car():\n",
    "    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data'\n",
    "    toNum = {\n",
    "        'low': 1,\n",
    "        'med': 2,\n",
    "        'high': 3,\n",
    "        'vhigh':4,\n",
    "        '5more':5,\n",
    "        'more':5,\n",
    "        'small':1,\n",
    "        'big':3\n",
    "    }\n",
    "    dataset = pd.read_csv(url, header=None)\n",
    "    y = dataset[6].to_numpy()\n",
    "    dataset = dataset.drop([6], axis=1)\n",
    "    dataset = dataset.replace({0:toNum, 1:toNum, 2:toNum, 3:toNum, 4:toNum, 5:toNum})\n",
    "    dataset = dataset.apply(pd.to_numeric)\n",
    "    \n",
    "    isInB = np.array([dataset.to_numpy()[i,3]>3 for i in range(len(dataset))])\n",
    "    isInB = isInB.reshape(len(isInB), 1)\n",
    "    #dataset = dataset.drop([3], axis=1)\n",
    "    X = np.append(dataset.to_numpy(), isInB, axis=1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def bias_car(data, labels):\n",
    "    isInB = data[:,-1]\n",
    "    X = data[isInB==1]\n",
    "    X = X[:, 0:(len(data[0])-1)]\n",
    "    y = labels[isInB==1]\n",
    "    return X, y\n",
    "\n",
    "def cardio():\n",
    "    dataset = pd.read_csv('Imitate/Datasets/cardio_train.csv', header=0, sep=';', index_col=0)\n",
    "    y = dataset['cardio'].to_numpy()\n",
    "    dataset = dataset[['age', 'weight']]\n",
    "    dataset = dataset.assign(age = dataset.age/365.25)\n",
    "    \n",
    "    isInB = np.array([1] * len(dataset))\n",
    "    isInB = isInB.reshape(len(isInB), 1)\n",
    "    \n",
    "    X = np.append(dataset.to_numpy(), isInB, axis=1)\n",
    "    \n",
    "    # draw sample\n",
    "    X, _, y, _ = train_test_split(X, y, test_size=0.90)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def bias_cardio(data, labels):\n",
    "    isInB = data[:,-1]\n",
    "    X = data[isInB==1]\n",
    "    X = X[:, 0:(len(data[0])-1)]\n",
    "    y = labels[isInB==1]\n",
    "    return X, y\n",
    "\n",
    "def shuttle(dataset_size=58000):\n",
    "    dataset1 = pd.read_csv('Imitate/Datasets/shuttle.trn', header=None,sep='\\s')\n",
    "    dataset2 = pd.read_csv('Imitate/Datasets/shuttle.tst', header=None,sep='\\s')\n",
    "    dataset = np.concatenate((dataset1, dataset2))\n",
    "    y = dataset[:,-1] == 1\n",
    "    isInB = np.array([dataset[i,0]>54.5 for i in range(len(dataset))])\n",
    "    isInB = isInB.reshape(len(isInB), 1)\n",
    "    X = dataset[:,0:-1]\n",
    "    #pca = PCA(n_components=21).fit(X)\n",
    "    #X = pca.transform(X)\n",
    "    X = np.append(X, isInB, axis=1)\n",
    "    \n",
    "    # draw sample\n",
    "    X, _, y, _ = train_test_split(X, y, test_size=0.8)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def bias_shuttle(data, labels):\n",
    "    isInB = data[:,-1]\n",
    "    X = data[isInB==1]\n",
    "    X = X[:, 0:(len(data[0])-1)]\n",
    "    y = labels[isInB==1]\n",
    "    return X, y\n",
    "\n",
    "def skin(dataset_size=4177):\n",
    "    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00229/Skin_NonSkin.txt'\n",
    "    dataset = pd.read_csv(url, header=None, sep='\\t')\n",
    "    y = dataset[3].to_numpy()\n",
    "    isInB = np.array([dataset.to_numpy()[i,2]<=170.5 for i in range(len(dataset))])\n",
    "    isInB = isInB.reshape(len(isInB), 1)\n",
    "    X = dataset.drop([2,3], axis=1).to_numpy()\n",
    "    #pca = PCA(n_components=21).fit(X)\n",
    "    #X = pca.transform(X)\n",
    "    X = np.append(X, isInB, axis=1)\n",
    "    \n",
    "    # draw sample\n",
    "    X, _, y, _ = train_test_split(X, y, test_size=0.95)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def bias_skin(data, labels):\n",
    "    isInB = data[:,-1]\n",
    "    X = data[isInB==1]\n",
    "    X = X[:, 0:(len(data[0])-1)]\n",
    "    y = labels[isInB==1]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_methods = {\n",
    "    \"random\": random_batch,\n",
    "    \"uncertainty\": batch.uncertainty_batch_sampling,\n",
    "    \"uncertainty_id\": uncertainty_id,\n",
    "    \"fgm\": fgm,\n",
    "    # deepfool is **slow**\n",
    "    \"deepfool\": deepfool\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-20fced1c9568>:110: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  dataset1 = pd.read_csv('Imitate/Datasets/shuttle.trn', header=None,sep='\\s')\n",
      "<ipython-input-5-20fced1c9568>:111: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  dataset2 = pd.read_csv('Imitate/Datasets/shuttle.tst', header=None,sep='\\s')\n"
     ]
    }
   ],
   "source": [
    "test_datasets = {\n",
    "    **{fn.__name__: fn() for fn in [banknote, haberman, car, cardio, skin, shuttle, abalone, digits]},\n",
    "    \n",
    "    \"banknote biased\": bias_banknote(*banknote()),\n",
    "    \"car biased\": bias_car(*car()), \n",
    "    \"cardio biased\": bias_cardio(*cardio()), \n",
    "    \"skin biased\": bias_skin(*skin()), \n",
    "    \"shuttle biased\": bias_shuttle(*shuttle()), \n",
    "    \"abalone biased\": bias_abalone(*abalone()), \n",
    "    #\"digits biased\": bias_digits(*digits())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variance**\n",
    "\n",
    "Could probably only do one run of deepfool as the variance seems pretty low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annoying hack so that the progressbars disapear as they're supposed to\n",
    "from IPython.core.display import HTML, display\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    ".p-Widget.jp-OutputPrompt.jp-OutputArea-prompt:empty {\n",
    "  padding: 0;\n",
    "  border: 0;\n",
    "}\n",
    ".p-Widget.jp-RenderedText.jp-OutputArea-output pre:empty {\n",
    "  display: none;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "import libutil; reload(libutil)\n",
    "\n",
    "for dataset_name, (X, y) in tqdm(test_datasets.items(), desc=\"Datasets\", leave=False):\n",
    "    X_labelled, X_unlabelled, y_labelled, y_oracle, X_test, y_test = active_split(X, y, labeled_size=0.1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18,4))\n",
    "\n",
    "    for method_name, method in tqdm(query_methods.items(), desc=f\"Method\", leave=False):\n",
    "        #try:\n",
    "            #metrics = pd.read_csv(f\"Experiments/experiment_real_{dataset_name}_method_{method_name}.csv\")\n",
    "            #stderr = pd.read_csv(f\"Experiments/experiment_real_{dataset_name}_method_{method_name}_stderr.csv\")\n",
    "        #except FileNotFoundError:\n",
    "        metrics = ProgressParallel(n_jobs=4, total=(10 if method_name != \"deepfool\" or method_name != \"fgm\" else 4), desc=f\"Run\", leave=False)(\n",
    "            delayed(\n",
    "                lambda X_labelled, X_unlabelled, y_labelled, y_oracle, X_test, y_test, method: MyActiveLearner().active_learn2(X_labelled, X_unlabelled, y_labelled, y_oracle, X_test, y_test, method)\n",
    "            )(X_labelled, X_unlabelled, y_labelled, y_oracle, X_test, y_test, partial(method, n_instances=10))\n",
    "            for _ in range((10 if method_name != \"deepfool\" or method_name != \"fgm\" else 4))\n",
    "        )\n",
    "        metrics, stderr = metrics[0].average(metrics[1:])\n",
    "        metrics.to_csv(f\"Experiments/experiment_real_{dataset_name}_method_{method_name}.csv\")\n",
    "        stderr.to_csv(f\"Experiments/experiment_real_{dataset_name}_method_{method_name}_stderr.csv\")\n",
    "\n",
    "        for i, ax in enumerate(axes.flatten()):\n",
    "            ax.errorbar(metrics['x'], metrics.iloc[:,1+i], yerr=stderr.iloc[:,1+i], label=f\"{method_name}\" if i == 0 else \"\")\n",
    "            ax.set_xlabel(\"Instances\"); ax.set_ylabel([\"Accuracy\", \"F1\", \"AUC ROC\"][i]); plt.suptitle(f\"{dataset_name}\")\n",
    "\n",
    "    fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_methods = {\n",
    "    #\"random\": random_batch,\n",
    "    #\"uncertainty\": batch.uncertainty_batch_sampling,\n",
    "    #\"uncertainty_id\": uncertainty_id,\n",
    "    \"fgm\": fgm,\n",
    "    # deepfool is **slow**\n",
    "    \"deepfool\": deepfool\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annoying hack so that the progressbars disapear as they're supposed to\n",
    "from IPython.core.display import HTML, display\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    ".p-Widget.jp-OutputPrompt.jp-OutputArea-prompt:empty {\n",
    "  padding: 0;\n",
    "  border: 0;\n",
    "}\n",
    ".p-Widget.jp-RenderedText.jp-OutputArea-output pre:empty {\n",
    "  display: none;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "import libutil; reload(libutil)\n",
    "\n",
    "for dataset_name, (X, y) in tqdm(test_datasets.items(), desc=\"Datasets\", leave=False):\n",
    "    X_labelled, X_unlabelled, y_labelled, y_oracle, X_test, y_test = active_split(X, y, labeled_size=0.1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(18,4))\n",
    "\n",
    "    for method_name, method in tqdm(query_methods.items(), desc=f\"Method\", leave=False):\n",
    "        #try:\n",
    "            #metrics = pd.read_csv(f\"Experiments/experiment_real_{dataset_name}_method_{method_name}.csv\")\n",
    "            #stderr = pd.read_csv(f\"Experiments/experiment_real_{dataset_name}_method_{method_name}_stderr.csv\")\n",
    "        #except FileNotFoundError:\n",
    "        for teach_adversarial in tqdm([True, False], desc=\"Teach adversarial\", leave=False):\n",
    "            metrics = ProgressParallel(n_jobs=4, total=(10 if method_name != \"deepfool\" or method_name != \"fgm\" else 4), desc=f\"Run\", leave=False)(\n",
    "                delayed(\n",
    "                    lambda X_labelled, X_unlabelled, y_labelled, y_oracle, X_test, y_test, method, teach_adversarial: MyActiveLearner(metrics=[accuracy_score, f1_score, roc_auc_score, empirical_robustness]).active_learn2(X_labelled, X_unlabelled, y_labelled, y_oracle, X_test, y_test, method, teach_adversarial=teach_adversarial)\n",
    "                )(X_labelled, X_unlabelled, y_labelled, y_oracle, X_test, y_test, partial(method, n_instances=10), teach_adversarial)\n",
    "                for _ in range((10 if method_name != \"deepfool\" or method_name != \"fgm\" else 4))\n",
    "            )\n",
    "            metrics, stderr = metrics[0].average(metrics[1:])\n",
    "            metrics.to_csv(f\"Experiments/experiment_real_{dataset_name}_method_{method_name}.csv\")\n",
    "            stderr.to_csv(f\"Experiments/experiment_real_{dataset_name}_method_{method_name}_stderr.csv\")\n",
    "\n",
    "            for i, ax in enumerate(axes.flatten()):\n",
    "                ax.errorbar(metrics['x'], metrics.iloc[:,1+i], yerr=stderr.iloc[:,1+i], label=f\"{method_name}\" if i == 0 else \"\")\n",
    "                ax.set_xlabel(\"Instances\"); ax.set_ylabel([\"Accuracy\", \"F1\", \"AUC ROC\", \"Robustness\"][i]); plt.suptitle(f\"{dataset_name}\")\n",
    "\n",
    "    fig.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
