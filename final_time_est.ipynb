{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from importlib import reload\n",
    "import librun\n",
    "import libdatasets\n",
    "from libadversarial import uncertainty_stop\n",
    "from time import monotonic\n",
    "from sklearn.svm import SVC\n",
    "from libactive import expected_error\n",
    "from modAL.models import ActiveLearner\n",
    "from libadversarial import adversarial\n",
    "from art.attacks.evasion import DeepFool\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def pluralize(n, word):\n",
    "    if n == 1:\n",
    "        return '%d %s' % (n, word)\n",
    "        \n",
    "    return '%d %ss' % (n, word)\n",
    "        \n",
    "def format_duration(seconds):\n",
    "    if seconds == 0:\n",
    "        return \"now\"\n",
    "    \n",
    "    ONE_MINUTE = 60\n",
    "    ONE_HOUR = 60 * ONE_MINUTE\n",
    "    ONE_DAY = 24 * ONE_HOUR\n",
    "    ONE_YEAR = 365 * ONE_DAY\n",
    "    \n",
    "    units = (\n",
    "        (ONE_YEAR, 'year'),\n",
    "        (ONE_DAY, 'day'),\n",
    "        (ONE_HOUR, 'hour'),\n",
    "        (ONE_MINUTE, 'minute'),\n",
    "        (1, 'second'),\n",
    "    )\n",
    "        \n",
    "    r = []\n",
    "    for unit in units:\n",
    "        time_period, word = unit\n",
    "        if seconds >= time_period:\n",
    "            n = int(seconds / time_period)\n",
    "            r.append(pluralize(n, word))\n",
    "            seconds -= n * time_period\n",
    "    \n",
    "    return ' and'.join(', '.join(r).rsplit(',', 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't tweak these, they were the values used to record the hardcoded times below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 100\n",
    "n_datasets = 40\n",
    "n_query_methods = 2\n",
    "n_models = 1\n",
    "n_parameter_combinations = 1\n",
    "stop_size = 1000\n",
    "batch_size = 10\n",
    "\n",
    "total_runs = n_runs*n_datasets*n_query_methods*n_models*n_parameter_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct = []\n",
    "fit = []\n",
    "inference = []\n",
    "ee = []\n",
    "deepfool = []\n",
    "construct = np.array([134.40000000009604, 146.79999999989377, 137.60000000002037, 131.40000000003056, 996.9999999999345, 6852.999999999884, 940.6000000000859, 1134.400000000096, 1222.0000000001164, 196.80000000007567, 1068.7999999998283])/total_runs*n_datasets\n",
    "fit = np.array([80939.99999999141, 65000.0, 51239.99999999796, 45939.999999991414, 50920.00000000553, 53440.000000009604, 38760.00000000204, 410319.99999999243, 44059.999999990396, 8439.999999973224, 26802180.00000001])/total_runs*n_datasets\n",
    "# Assuming 1000 unlabelled instances are subsampled\n",
    "inference = np.array([14376.525522657837, 12204.181258622519, 9850.36612543792, 7693.9403586968065, 8360.520826340678, 10416.765896641573, 6349.36374153124, 55546.66666666677, 7694.299167411302, 32.413793103152216])/total_runs*n_datasets\n",
    "deepfool = np.array([176561999.99999988, 111873999.99999979, 108250000.0, 91280000.00000064, 100626000.00000021, 219467999.99999893, 77811999.9999999, 3773405999.999999, 156126000.00000024, 17155999.99999904])/total_runs*n_datasets\n",
    "ee = np.array([25940000.000009604, 27200000.00001164, 18119999.999998983, 19380000.000001017, 35300000.00000655, 1293440000.0000098, 3439999.999991414, 313119999.999999, 39360000.00001513, 639999.9999848661])/total_runs*n_datasets\n",
    "inference_unsub = np.array([270940.0000000096, 230000.0, 185640.00000000306, 145000.0, 6725320.00000001, 43739.99999999796, 38420.00000000553, 3332800.0000000065, 4279700.000000012, 1879.9999999828287, 270940.0000000096])/total_runs*n_datasets\n",
    "ee_unsub = np.array([3327487451.9999976, 2108377403.9999962, 2040079500.0, 1720262880.0000122, 80944963164.00017, 921546131.9999955, 470840411.99999934, 226404359999.99994, 86839935342.00012, 995047999.9999443])/total_runs*n_datasets\n",
    "deepfool_unsub = np.array([488865240.000181, 512611200.0002194, 341489519.9999808, 365235480.0000192, 28395814200.00527, 5431154560.000041, 20815439.999948047, 18787199999.99994, 21892701120.00842, 37119999.99912223, 31235205120.001183])/total_runs*n_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runtime(n_cores, n_datasets, n_runs, include_deepfool, include_ee, subsample, subsample_pool):\n",
    "    time = sum((sum(construct), sum(fit), sum(inference), ((sum(ee)/1000*subsample_pool) if subsample else sum(ee_unsub)) if include_ee else 0, ((sum(deepfool)/1000*subsample_pool) if subsample else sum(deepfool_unsub)) if include_deepfool else 0))/len(construct)*n_datasets/n_cores*n_runs\n",
    "    print(format_duration(time))\n",
    "    print(f\"{time*n_cores/60/60:.0f} CPU hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679e13c1c56647598f26c09aeb6ec74e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=100, description='# cores', max=1000, min=1), IntSlider(value=20, descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(\n",
    "    runtime, \n",
    "    include_deepfool=widgets.Checkbox(description=\"Include deepfool query method\"),\n",
    "    n_datasets=widgets.IntSlider(20,1,50, description=\"# datasets\"),\n",
    "    n_cores=widgets.IntSlider(100,1,1000, description=\"# cores\"),\n",
    "    include_ee=widgets.Checkbox(description=\"Include expected error\"),\n",
    "    subsample_pool=widgets.IntSlider(1000,10,10000, description=\"# unlabelled instances to sample\"),\n",
    "    n_runs=widgets.IntSlider(100,1,100, description=\"# runs\"),\n",
    "    subsample=widgets.Checkbox(True, description=\"Subsample\")\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected error and deepfool are **slow**. It might be feasable to run one or both if the number of repetitions can be reduced to ~10-30, but probably only on a smaller number of datasets and only if using significant subsampling on the unlabelled pool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def wrap(func, *args, **kwargs):\n",
    "    wrapper = lambda: lru_cache()(func)(*args, **kwargs)\n",
    "    for attr in [attr for attr in dir(func) if not attr.startswith('__')]:\n",
    "        setattr(wrapper, attr, getattr(func, attr))\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(libdatasets); from libdatasets import *\n",
    "matrix = {\n",
    "    # Dataset fetchers should cache if possible\n",
    "    # Lambda wrapper required for function to be pickleable (sent to other threads via joblib)\n",
    "    \"datasets\": [\n",
    "        # Text classification\n",
    "        \n",
    "        # https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.6090&rep=rep1&type=pdf\n",
    "        (\"newsgroups_faith\", wrap(newsgroups, 1000, ('alt.atheism', 'soc.religion.christian'))),\n",
    "        (\"newsgroups_graphics\", wrap(newsgroups, 1000, ('comp.graphics', 'comp.windows.x'))),\n",
    "        (\"newsgroups_hardware\", wrap(newsgroups, 1000, ('comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware'))),\n",
    "        (\"newsgroups_sports_crypto\", wrap(newsgroups, 1000, ('rec.sport.baseball', 'sci.crypt'))),\n",
    "    \n",
    "        (\"rcv1\", wrap(rcv1, 1000)),\n",
    "        (\"webkb\", wrap(webkb, 1000)),\n",
    "        (\"spamassassin\", wrap(spamassassin, 1000)),\n",
    "        \n",
    "        # Image classification\n",
    "        (\"cifar10\", wrap(cifar10, 1000)),\n",
    "        (\"quickdraw\", wrap(quickdraw, 1000)),\n",
    "        (\"avila\", wrap(avila, 1000)),\n",
    "        \n",
    "        # General\n",
    "        (\"shuttle\", wrap(shuttle, 1000)),\n",
    "        #(\"covertype\", wrap(covertype, None)), # fit takes a million years (1233s for 1000 instances)\n",
    "        (\"smartphone\", wrap(smartphone, 1000)),\n",
    "        #(\"ida2016\", wrap(ida2016, None)), # HAS MISSING VALUES\n",
    "        (\"htru2\", wrap(htru2, 1000)),\n",
    "        #(\"malware\", wrap(malware, None)), # MALWARE FIT DID NOT FINISH (07:30:30.xxx CPU time)\n",
    "        (\"bidding\", wrap(bidding, 1000)),\n",
    "        (\"swarm\", wrap(swarm, 1000)),\n",
    "        (\"bank\", wrap(bank, 1000)),\n",
    "        (\"buzz\", wrap(buzz, 1000)), # Slow fit times\n",
    "        (\"sensorless\", wrap(sensorless, 1000)),\n",
    "        (\"dota2\", wrap(dota2, 1000)),\n",
    "        \n",
    "        # Bio\n",
    "        (\"abalone\", wrap(abalone, 1000)),\n",
    "        (\"splice\", wrap(splice, 1000)),\n",
    "        (\"anuran\", wrap(anuran, 1000)),\n",
    "        \n",
    "        # Medical\n",
    "        (\"cardio\", wrap(cardio, 1000)),\n",
    "        (\"skin\", wrap(skin, 1000)),\n",
    "        \n",
    "    ],\n",
    "    \"dataset_mutators\": {\n",
    "        \"none\": (lambda *x, **kwargs: x),\n",
    "    },\n",
    "    \"methods\": [\n",
    "        (\"uncertainty\", partial(uncertainty_stop, n_instances=10)),\n",
    "    ],\n",
    "    \"models\": [\n",
    "        \"svm-linear\"\n",
    "    ],\n",
    "    \"meta\": {\n",
    "        \"dataset_size\": 1000,\n",
    "        \"labelled_size\": 10,\n",
    "        \"test_size\": {\n",
    "            \"newsgroups_faith\": 500,\n",
    "            \"newsgroups_graphics\": 500,\n",
    "            \"newsgroups_hardware\": 500,\n",
    "            \"newsgroups_sports_crypto\": 500,\n",
    "            \"*\": 0.5\n",
    "        },\n",
    "        \"n_runs\": 10,\n",
    "        \"ret_classifiers\": True,\n",
    "        \"ensure_y\": True,\n",
    "        \"stop_info\": True,\n",
    "        \"aggregate\": False,\n",
    "        \"stop_function\": (\"len1000\", lambda learner: learner.y_training.shape[0] >= 1000),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upper bound on total runtime:\n",
    "n_runs = 100\n",
    "n_datasets = 40\n",
    "n_query_methods = 2 # ?\n",
    "n_models = 1\n",
    "n_parameter_combinations = 1\n",
    "stop_size = 1000\n",
    "batch_size = 10\n",
    "\n",
    "total_runs = n_runs*n_datasets*n_query_methods*n_models*n_parameter_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'libactive' from 'C:\\\\Users\\\\Zac\\\\Programming\\\\python\\\\research\\\\libactive.py'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(libactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def pluralize(n, word):\n",
    "    if n == 1:\n",
    "        return '%d %s' % (n, word)\n",
    "        \n",
    "    return '%d %ss' % (n, word)\n",
    "        \n",
    "def format_duration(seconds):\n",
    "    if seconds == 0:\n",
    "        return \"now\"\n",
    "    \n",
    "    ONE_MINUTE = 60\n",
    "    ONE_HOUR = 60 * ONE_MINUTE\n",
    "    ONE_DAY = 24 * ONE_HOUR\n",
    "    ONE_YEAR = 365 * ONE_DAY\n",
    "    \n",
    "    units = (\n",
    "        (ONE_YEAR, 'year'),\n",
    "        (ONE_DAY, 'day'),\n",
    "        (ONE_HOUR, 'hour'),\n",
    "        (ONE_MINUTE, 'minute'),\n",
    "        (1, 'second'),\n",
    "    )\n",
    "        \n",
    "    r = []\n",
    "    for unit in units:\n",
    "        time_period, word = unit\n",
    "        if seconds >= time_period:\n",
    "            n = int(seconds / time_period)\n",
    "            r.append(pluralize(n, word))\n",
    "            seconds -= n * time_period\n",
    "    \n",
    "    return ' and'.join(', '.join(r).rsplit(',', 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct = []\n",
    "fit = []\n",
    "inference = []\n",
    "ee = []\n",
    "deepfool = []\n",
    "construct = np.array([134.40000000009604, 146.79999999989377, 137.60000000002037, 131.40000000003056, 996.9999999999345, 6852.999999999884, 940.6000000000859, 1134.400000000096, 1222.0000000001164, 196.80000000007567, 1068.7999999998283])/total_runs*n_datasets\n",
    "fit = np.array([80939.99999999141, 65000.0, 51239.99999999796, 45939.999999991414, 50920.00000000553, 53440.000000009604, 38760.00000000204, 410319.99999999243, 44059.999999990396, 8439.999999973224, 26802180.00000001])/total_runs*n_datasets\n",
    "# Assuming 1000 unlabelled instances are subsampled\n",
    "inference = np.array([14376.525522657837, 12204.181258622519, 9850.36612543792, 7693.9403586968065, 8360.520826340678, 10416.765896641573, 6349.36374153124, 55546.66666666677, 7694.299167411302, 32.413793103152216])/total_runs*n_datasets\n",
    "deepfool = np.array([176561999.99999988, 111873999.99999979, 108250000.0, 91280000.00000064, 100626000.00000021, 219467999.99999893, 77811999.9999999, 3773405999.999999, 156126000.00000024, 17155999.99999904])/total_runs*n_datasets\n",
    "ee = np.array([25940000.000009604, 27200000.00001164, 18119999.999998983, 19380000.000001017, 35300000.00000655, 1293440000.0000098, 3439999.999991414, 313119999.999999, 39360000.00001513, 639999.9999848661])/total_runs*n_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runtime(n_cores, n_datasets, n_runs, subsample_pool, include_deepfool, include_ee):\n",
    "    print(format_duration(sum((sum(construct), sum(fit), sum(inference), sum(ee)*1000/subsample_pool if include_ee else 0, sum(deepfool)*1000/subsample_pool if include_deepfool else 0))/len(construct)*n_datasets/n_cores*n_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=100, description='# cores', max=1000, min=1), IntSlider(value=20, descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(\n",
    "    runtime, \n",
    "    include_deepfool=widgets.Checkbox(description=\"Include deepfool query method\"),\n",
    "    n_datasets=widgets.IntSlider(20,1,50, description=\"# datasets\"),\n",
    "    n_cores=widgets.IntSlider(100,1,1000, description=\"# cores\"),\n",
    "    include_ee=widgets.Checkbox(description=\"Include expected error\"),\n",
    "    subsample_pool=widgets.IntSlider(1000,10,10000, description=\"# unlabelled instances to sample\"),\n",
    "    n_runs=widgets.IntSlider(100,1,100, description=\"# runs\"),\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cores = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 100 cores:\n",
      "  Estimated runtime for 100 runs on 40 datasets with 2 query methods: 591 years, 330 days, 13 hours, 56 minutes and 49 seconds\n",
      "  Without deepfool: 467 years, 343 days, 24 minutes and 46 secondsd\n",
      "  Without ee: 124 years, 5 days, 17 hours, 46 minutes and 56 secondsd\n",
      "  Without deepfool and ee: 18 days, 4 hours, 14 minutes and 53 secondsd\n"
     ]
    }
   ],
   "source": [
    "total_t_est = sum((sum(construct), sum(fit), sum(inference), sum(ee), sum(deepfool)))/len(construct)*n_datasets/n_cores\n",
    "t_est_no_ee = sum((sum(construct), sum(fit), sum(inference), sum(deepfool)))/len(construct)*n_datasets/n_cores\n",
    "t_est_no_deep = sum((sum(construct), sum(fit), sum(inference), sum(ee)))/len(construct)*n_datasets/n_cores\n",
    "t_est_no_deep_no_ee = sum((sum(construct), sum(fit), sum(inference)))/len(construct)*n_datasets/n_cores\n",
    "print(f\"Using {n_cores} cores:\")\n",
    "print(f\"  Estimated runtime for {n_runs} runs on {n_datasets} datasets with {n_query_methods} query methods: {format_duration(total_t_est)}\")\n",
    "print(f\"  Without deepfool: {format_duration(t_est_no_deep)}d\")\n",
    "print(f\"  Without ee: {format_duration(t_est_no_ee)}d\")\n",
    "print(f\"  Without deepfool and ee: {format_duration(t_est_no_deep_no_ee)}d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total fit time: 320 days, 53 minutes and 59 seconds\n",
      "Total inference time: 179 days, 16 hours, 19 minutes and 40 seconds\n",
      "Total ee time: 12866 years, 355 days, 14 hours, 31 minutes and 26 seconds\n",
      "Total deepfool time: 3409 years, 23 days, 11 minutes and 20 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total fit time: {format_duration(sum(fit))}\")\n",
    "print(f\"Total inference time: {format_duration(sum(inference))}\")\n",
    "print(f\"Total ee time: {format_duration(sum(ee))}\")\n",
    "print(f\"Total deepfool time: {format_duration(sum(deepfool))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options for running EE & deepfool:\n",
    "\n",
    "* Subsamble the unlabelled pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zac\\Programming\\python\\research\\libdatasets.py:197: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  dataset1 = pd.read_csv(\"Imitate/Datasets/shuttle.trn\", header=None, sep=\"\\s\")\n",
      "C:\\Users\\Zac\\Programming\\python\\research\\libdatasets.py:198: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  dataset2 = pd.read_csv(\"Imitate/Datasets/shuttle.tst\", header=None, sep=\"\\s\")\n"
     ]
    }
   ],
   "source": [
    "inference_sub = []\n",
    "ee_sub = []\n",
    "deepfool_sub = []\n",
    "for i, (name, dataset) in enumerate(matrix['datasets'][:10]):\n",
    "    x_all = getattr(libdatasets, name if not name.startswith(\"newsgroups\") else \"newsgroups\")(dataset_size=None)[0]\n",
    "    inference_sub.append(inference[i]/x_all.shape[0]*1000)\n",
    "    ee_sub.append(ee[i]/x_all.shape[0]*1000)\n",
    "    deepfool_sub.append(deepfool[i]/x_all.shape[0]*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14376.525522657837, 12204.181258622519, 9850.36612543792, 7693.9403586968065, 8360.520826340678, 10416.765896641573, 6349.36374153124, 55546.66666666677, 7694.299167411302, 32.413793103152216]\n",
      "[176561999.99999988, 111873999.99999979, 108250000.0, 91280000.00000064, 100626000.00000021, 219467999.99999893, 77811999.9999999, 3773405999.999999, 156126000.00000024, 17155999.99999904]\n",
      "[25940000.000009604, 27200000.00001164, 18119999.999998983, 19380000.000001017, 35300000.00000655, 1293440000.0000098, 3439999.999991414, 313119999.999999, 39360000.00001513, 639999.9999848661]\n"
     ]
    }
   ],
   "source": [
    "print(inference_sub)\n",
    "print(ee_sub)\n",
    "print(deepfool_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total fit time: 320 days, 53 minutes and 59 seconds\n",
      "Subsampled inference time: 1 day, 12 hours, 48 minutes and 45 seconds\n",
      "Subsampled ee time: 153 years, 87 days, 9 hours, 46 minutes and 39 seconds\n",
      "Subsampled deepfool time: 56 years, 114 days, 20 hours and 40 minutes\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total fit time: {format_duration(sum(fit))}\")\n",
    "print(f\"Subsampled inference time: {format_duration(sum(inference_sub))}\")\n",
    "print(f\"Subsampled ee time: {format_duration(sum(ee_sub))}\")\n",
    "print(f\"Subsampled deepfool time: {format_duration(sum(deepfool_sub))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cores=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2000 cores and subsampling:\n",
      "  Estimated runtime for 100 runs on 40 datasets with 2 query methods: 139 days, 15 hours, 39 minutes and 54 seconds\n",
      "  Without deepfool: 102 days, 6 hours, 43 minutes and 32 secondsd\n",
      "  Without ee: 37 days, 22 hours, 58 minutes and 41 secondsd\n",
      "  Without deepfool and ee: 14 hours, 2 minutes and 19 secondsd\n"
     ]
    }
   ],
   "source": [
    "total_t_est = sum((sum(construct), sum(fit), sum(inference_sub), sum(ee_sub), sum(deepfool_sub)))/len(construct)*n_datasets/n_cores\n",
    "t_est_no_ee = sum((sum(construct), sum(fit), sum(inference_sub), sum(deepfool_sub)))/len(construct)*n_datasets/n_cores\n",
    "t_est_no_deep = sum((sum(construct), sum(fit), sum(inference_sub), sum(ee_sub)))/len(construct)*n_datasets/n_cores\n",
    "t_est_no_deep_no_ee = sum((sum(construct), sum(fit), sum(inference_sub)))/len(construct)*n_datasets/n_cores\n",
    "print(f\"Using {n_cores} cores and subsampling:\")\n",
    "print(f\"  Estimated runtime for {n_runs} runs on {n_datasets} datasets with {n_query_methods} query methods: {format_duration(total_t_est)}\")\n",
    "print(f\"  Without deepfool: {format_duration(t_est_no_deep)}d\")\n",
    "print(f\"  Without ee: {format_duration(t_est_no_ee)}d\")\n",
    "print(f\"  Without deepfool and ee: {format_duration(t_est_no_deep_no_ee)}d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct=construct[:-1]\n",
    "#fit=fit[:-1]\n",
    "#inference=inference[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 10, 10, 10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(construct),len(fit),len(inference),len(ee),len(deepfool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[134.40000000009604, 146.79999999989377, 137.60000000002037, 131.40000000003056, 996.9999999999345, 6852.999999999884, 940.6000000000859, 1134.400000000096, 1222.0000000001164, 196.80000000007567, 1068.7999999998283]\n",
      "[80939.99999999141, 65000.0, 51239.99999999796, 45939.999999991414, 50920.00000000553, 53440.000000009604, 38760.00000000204, 410319.99999999243, 44059.999999990396, 8439.999999973224, 26802180.00000001]\n",
      "[270940.0000000096, 230000.0, 185640.00000000306, 145000.0, 6725320.00000001, 43739.99999999796, 38420.00000000553, 3332800.0000000065, 4279700.000000012, 1879.9999999828287, 270940.0000000096]\n",
      "[3327487451.9999976, 2108377403.9999962, 2040079500.0, 1720262880.0000122, 80944963164.00017, 921546131.9999955, 470840411.99999934, 226404359999.99994, 86839935342.00012, 995047999.9999443]\n",
      "[488865240.000181, 512611200.0002194, 341489519.9999808, 365235480.0000192, 28395814200.00527, 5431154560.000041, 20815439.999948047, 18787199999.99994, 21892701120.00842, 37119999.99912223, 31235205120.001183]\n"
     ]
    }
   ],
   "source": [
    "print(construct)\n",
    "print(fit)\n",
    "print(inference)\n",
    "print(ee)\n",
    "print(deepfool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct = []\n",
    "fit = []\n",
    "inference = []\n",
    "ee = []\n",
    "deepfool = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thundersvm import SVC as ThunderSVC\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libactive import expected_error; from libadversarial import adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished newsgroups_faith\n",
      "Finished newsgroups_graphics\n",
      "Finished newsgroups_hardware\n",
      "Finished newsgroups_sports_crypto\n",
      "Finished rcv1\n",
      "Finished webkb\n",
      "Finished spamassassin\n",
      "Finished cifar10\n",
      "Finished quickdraw\n",
      "Finished avila\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zac\\Programming\\python\\research\\libdatasets.py:199: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  dataset1 = pd.read_csv(\"Imitate/Datasets/shuttle.trn\", header=None, sep=\"\\s\")\n",
      "C:\\Users\\Zac\\Programming\\python\\research\\libdatasets.py:200: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  dataset2 = pd.read_csv(\"Imitate/Datasets/shuttle.tst\", header=None, sep=\"\\s\")\n",
      "C:\\Users\\Zac\\Programming\\python\\research\\libdatasets.py:199: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  dataset1 = pd.read_csv(\"Imitate/Datasets/shuttle.trn\", header=None, sep=\"\\s\")\n",
      "C:\\Users\\Zac\\Programming\\python\\research\\libdatasets.py:200: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  dataset2 = pd.read_csv(\"Imitate/Datasets/shuttle.tst\", header=None, sep=\"\\s\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished shuttle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zac\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished smartphone\n",
      "Finished htru2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X.shape[1] = 20719 should be equal to 4540, the number of features at training time",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2a7d786a9b44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmonotonic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0minference\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mstop_size\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zac\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_predict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 661\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    662\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobA_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobB_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m             raise NotFittedError(\"predict_proba is not available when fitted \"\n",
      "\u001b[1;32mc:\\users\\zac\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    482\u001b[0m                                  (X.shape[1], self.shape_fit_[0]))\n\u001b[0;32m    483\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape_fit_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m             raise ValueError(\"X.shape[1] = %d should be equal to %d, \"\n\u001b[0m\u001b[0;32m    485\u001b[0m                              \u001b[1;34m\"the number of features at training time\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m                              (X.shape[1], self.shape_fit_[1]))\n",
      "\u001b[1;31mValueError\u001b[0m: X.shape[1] = 20719 should be equal to 4540, the number of features at training time"
     ]
    }
   ],
   "source": [
    "for name, dataset in matrix['datasets']:\n",
    "    #clf = ThunderSVC(kernel='linear', probability=True)\n",
    "    clf = SVC(kernel='linear', probability=True)\n",
    "    \n",
    "    start = monotonic()\n",
    "    X, y = dataset()\n",
    "    if X.dtype != np.float64:\n",
    "        X = X.astype(np.float64)\n",
    "    construct.append((monotonic()-start))\n",
    "    \n",
    "    start = monotonic()\n",
    "    clf.fit(X,y)\n",
    "    fit.append((monotonic()-start)*stop_size/batch_size)\n",
    "    \n",
    "    x_all = getattr(libdatasets, name if not name.startswith(\"newsgroups\") else \"newsgroups\")(dataset_size=None)[0]\n",
    "    if x_all.dtype != np.float64:\n",
    "        x_all = x_all.astype(np.float64)\n",
    "    \n",
    "    start = monotonic()\n",
    "    clf.predict_proba(x_all)\n",
    "    inference.append((monotonic()-start)*stop_size/batch_size)\n",
    "    \n",
    "    print(f\"Finished {name}\")\n",
    "    continue\n",
    "    \n",
    "    learner = ActiveLearner(estimator=clf, X_training=X, y_training=y)\n",
    "    \n",
    "    start = monotonic()\n",
    "    adversarial(learner, x_all[np.random.choice(x_all.shape[0], 1)], partial(DeepFool, verbose=False), n_instances=10)\n",
    "    deepfool.append(((monotonic()-start)*stop_size/batch_size))\n",
    "    shapes.append(x_all.shape[0])\n",
    "    \n",
    "    start = monotonic()\n",
    "    expected_error(learner, x_all[np.random.choice(x_all.shape[0], 10)])\n",
    "    ee.append((monotonic()-start)*stop_size/batch_size)\n",
    "    \n",
    "    print(f\"Finished {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newsgroups_faith: construct: 1 fit: 403 inference: 1225\n",
      "newsgroups_graphics: construct: 1 fit: 281 inference: 1012\n",
      "newsgroups_hardware: construct: 1 fit: 256 inference: 1025\n",
      "newsgroups_sports_crypto: construct: 1 fit: 225 inference: 703\n",
      "rcv1: construct: 6 fit: 273 inference: 35777\n",
      "webkb: construct: 37 fit: 275 inference: 208\n",
      "spamassassin: construct: 5 fit: 194 inference: 180\n",
      "cifar10: construct: 6 fit: 1842 inference: 16519\n",
      "quickdraw: construct: 11 fit: 250 inference: 23594\n",
      "avila: construct: 1 fit: 509 inference: 1450\n",
      "shuttle: construct: 1 fit: 320 inference: 1084\n",
      "smartphone: construct: 1 fit: 280 inference: 983\n",
      "htru2: construct: 1 fit: 230 inference: 766\n",
      "bidding: construct: 6 fit: 275 inference: 36103\n",
      "swarm: construct: 38 fit: 284 inference: 223\n",
      "bank: construct: 5 fit: 203 inference: 188\n",
      "buzz: construct: 6 fit: 1994 inference: 17097\n",
      "sensorless: construct: 11 fit: 253 inference: 22008\n",
      "dota2: construct: 2 fit: 19 inference: 39\n",
      "abalone: construct: 1 fit: 45 inference: 9\n",
      "splice: construct: 2 fit: 75 inference: 222\n",
      "anuran: construct: 0 fit: 75 inference: 3\n"
     ]
    }
   ],
   "source": [
    "for c, f, i, (name, dataset) in zip(construct, fit, inference, matrix['datasets']):\n",
    "    print(f\"{name}: construct: {c:.0f} fit: {f:.0f} inference: {i:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, f, i, (name, dataset) in zip(construct, fit, inference, matrix['datasets']):\n",
    "    print(f\"{name}: construct: {c:.0f} fit: {f:.0f} inference: {i:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
